{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89aa2acf",
   "metadata": {},
   "source": [
    "# TWEETS SENTIMENT CLASSIFICATION USING LSTM\n",
    "\n",
    "Sentiment analysis refers to the idea of predicting the sentiment ( happy, sad, neutral) from a particular text. In this project, I will be performing sentiment analysis on a large real-world twitter dataset by applying techniques of NLP to make a binary classification (Positive and Negative). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5878ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING NECESSARY LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import textblob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import Word\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1ef8f",
   "metadata": {},
   "source": [
    "## Loading Dataset and Data Exploration for Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a5c0fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @GregAbbott_TX: @TedCruz: \"On my first day ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @warriorwoman91: I liked her and was happy ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  RT @ScottWalker: Didn't catch the full #GOPdeb...  Positive\n",
       "1  RT @RobGeorge: That Carly Fiorina is trending ...  Positive\n",
       "2  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Positive\n",
       "3  RT @GregAbbott_TX: @TedCruz: \"On my first day ...  Positive\n",
       "4  RT @warriorwoman91: I liked her and was happy ...  Negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading from csv file \n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac63c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Negative'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking target values we have\n",
    "\n",
    "data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0787b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding for the sentiment column. \n",
    "# Positive takes the value 1, negative takes the value 0\n",
    "\n",
    "data['sentiment'] = data['sentiment'].replace('Positive',1)\n",
    "data['sentiment'] = data['sentiment'].replace('Negative',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742e0734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8493\n",
       "1    2236\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the number counts of our unique classes\n",
    "\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b6c2f",
   "metadata": {},
   "source": [
    "# Text Cleaning and Preprocessing\n",
    "\n",
    "There is a lot of noise in the raw text data scrapped from the tweets. The critical part of text cleaning for sentiment analysis is to remove stop words.\n",
    "\n",
    "There are punctuations, symbols that will not contribute to our model much. There are also stop words present which need to be removed. Stop words refer to the connecting words like ‘the,’ ‘and’ ‘was,’ which do not provide any specific meaning, which will not help our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f94e1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb99216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to make necessary cleaning in the texts \n",
    "\n",
    "def clean(tweet):\n",
    "    tweet = tweet.lower() # Lowering all cases before continue\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) # Removing mentions @\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) # Removing http links \n",
    "    tweet = re.sub(\"[^A-Za-z]\",\" \", tweet) # Removing non-alphanumeric characters \n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = tweet.replace(\":\", \"\")\n",
    "    tweet = tweet.replace(\"rt\", \"\")\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "# removing stop words\n",
    "    temp = tweet.split()\n",
    "    temp = [w for w in temp if not w in stop_words]\n",
    "    tweet = \" \".join(word for word in temp)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8793f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Cleaning tweets calling clean() function\n",
    "    \n",
    "data['text'] = data['text'].map(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488479aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>catch full gopdebate last night scott best lin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>carly fiorina trending hours debate men comple...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gopdebate w delivered highest ratings history ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tx first day rescind every illegal executive a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>liked happy heard going moderator anymore gopd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deer headlights ben carson may brain surgeon p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>last night debate proved gopdebate batsask tbats</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fairness billclinton owns phrase gopdebate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>woke tweet gopdebate best line night via</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>reading family comments great gopdebate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  catch full gopdebate last night scott best lin...          1\n",
       "1  carly fiorina trending hours debate men comple...          1\n",
       "2  gopdebate w delivered highest ratings history ...          1\n",
       "3  tx first day rescind every illegal executive a...          1\n",
       "4  liked happy heard going moderator anymore gopd...          0\n",
       "5  deer headlights ben carson may brain surgeon p...          0\n",
       "6   last night debate proved gopdebate batsask tbats          0\n",
       "7         fairness billclinton owns phrase gopdebate          0\n",
       "8           woke tweet gopdebate best line night via          1\n",
       "9            reading family comments great gopdebate          0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f0b6b",
   "metadata": {},
   "source": [
    "# TOKENIZATION\n",
    "Tokenization refers to splitting the given sentence into a list of tokens, indexed or vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59683ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized_tweets'] = data.apply(lambda row : nltk.word_tokenize(str(row['text'])),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c43251c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>catch full gopdebate last night scott best lin...</td>\n",
       "      <td>1</td>\n",
       "      <td>[catch, full, gopdebate, last, night, scott, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>carly fiorina trending hours debate men comple...</td>\n",
       "      <td>1</td>\n",
       "      <td>[carly, fiorina, trending, hours, debate, men,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gopdebate w delivered highest ratings history ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[gopdebate, w, delivered, highest, ratings, hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tx first day rescind every illegal executive a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[tx, first, day, rescind, every, illegal, exec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>liked happy heard going moderator anymore gopd...</td>\n",
       "      <td>0</td>\n",
       "      <td>[liked, happy, heard, going, moderator, anymor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment  \\\n",
       "0  catch full gopdebate last night scott best lin...          1   \n",
       "1  carly fiorina trending hours debate men comple...          1   \n",
       "2  gopdebate w delivered highest ratings history ...          1   \n",
       "3  tx first day rescind every illegal executive a...          1   \n",
       "4  liked happy heard going moderator anymore gopd...          0   \n",
       "\n",
       "                                    tokenized_tweets  \n",
       "0  [catch, full, gopdebate, last, night, scott, b...  \n",
       "1  [carly, fiorina, trending, hours, debate, men,...  \n",
       "2  [gopdebate, w, delivered, highest, ratings, hi...  \n",
       "3  [tx, first, day, rescind, every, illegal, exec...  \n",
       "4  [liked, happy, heard, going, moderator, anymor...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ca31d",
   "metadata": {},
   "source": [
    "# Converting Tokenized Tweets to Vectors\n",
    "\n",
    "Keras has a pre-processing module for text, which offers us the tf.keras. pre-processing.text.Tokenizer() class. \n",
    "If we pass a list of texts to fit_on_texts() function, we will update the internal vocabulary accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "840ba385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9967 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(data.tokenized_tweets.values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bed23f",
   "metadata": {},
   "source": [
    "I will be applying a sequence model to this data. For this, I need to pass inputs of the same size. To achieve this, I will use the `pad_sequences()` function. This will return us sequences of a constant size, which can be passed as a parameter. I have set the sequence length as 30 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0316d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (10729, 30)\n",
      "Shape of data tensor: (10729,)\n"
     ]
    }
   ],
   "source": [
    "# Defining vocabulary size\n",
    "MAX_NB_WORDS = len(word_index) + 1\n",
    "\n",
    "# Max number of words in each tweets.\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "\n",
    "# Defining Embedding Dimention. This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data.tokenized_tweets)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "Y = data.sentiment\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of data tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adaa0fd",
   "metadata": {},
   "source": [
    "# Splitting Training and Testing Sets\n",
    "\n",
    "Before training my model, I need to divide my data into training and test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4b395e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7510, 30) (7510,)\n",
      "(3219, 30) (3219,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133f1ea",
   "metadata": {},
   "source": [
    "# MODELLING\n",
    "\n",
    "My architecture consists of three main parts. I start with the embedding layer defined previously, and it inputs the sequences and gives word embeddings. These embeddings are then passed on to the convolution layer, which will convert them into small feature vectors. Next, I have Dense (fully connected layers) for classification purposes. I use a sigmoid activation function before the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7adc4f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "from keras.layers import Activation, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "392fb853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 100)           996800    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,013,857\n",
      "Trainable params: 1,013,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building Model\n",
    "embedding_vector_length = 32 \n",
    "model = models.Sequential() \n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)) \n",
    "model.add(LSTM(units=embedding_vector_length, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85e20ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eeers\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7510 samples, validate on 3219 samples\n",
      "Epoch 1/15\n",
      "7510/7510 [==============================] - 2s 274us/step - loss: 0.5420 - accuracy: 0.7858 - val_loss: 0.5034 - val_accuracy: 0.7975\n",
      "Epoch 2/15\n",
      "7510/7510 [==============================] - 2s 223us/step - loss: 0.5158 - accuracy: 0.7891 - val_loss: 0.5020 - val_accuracy: 0.7975\n",
      "Epoch 3/15\n",
      "7510/7510 [==============================] - 2s 223us/step - loss: 0.4566 - accuracy: 0.8052 - val_loss: 0.3898 - val_accuracy: 0.8186\n",
      "Epoch 4/15\n",
      "7510/7510 [==============================] - 2s 224us/step - loss: 0.3149 - accuracy: 0.8783 - val_loss: 0.3860 - val_accuracy: 0.8434\n",
      "Epoch 5/15\n",
      "7510/7510 [==============================] - 2s 224us/step - loss: 0.2496 - accuracy: 0.9124 - val_loss: 0.4272 - val_accuracy: 0.8360\n",
      "Epoch 6/15\n",
      "7510/7510 [==============================] - 2s 223us/step - loss: 0.2114 - accuracy: 0.9285 - val_loss: 0.4072 - val_accuracy: 0.8431\n",
      "Epoch 7/15\n",
      "7510/7510 [==============================] - 2s 233us/step - loss: 0.1861 - accuracy: 0.9410 - val_loss: 0.4605 - val_accuracy: 0.8468\n",
      "Epoch 8/15\n",
      "7510/7510 [==============================] - 2s 226us/step - loss: 0.1616 - accuracy: 0.9535 - val_loss: 0.4963 - val_accuracy: 0.8416\n",
      "Epoch 9/15\n",
      "7510/7510 [==============================] - 2s 223us/step - loss: 0.1666 - accuracy: 0.9498 - val_loss: 0.4486 - val_accuracy: 0.8444\n",
      "Epoch 10/15\n",
      "7510/7510 [==============================] - 2s 248us/step - loss: 0.1524 - accuracy: 0.9554 - val_loss: 0.4628 - val_accuracy: 0.8515\n",
      "Epoch 11/15\n",
      "7510/7510 [==============================] - 2s 243us/step - loss: 0.1435 - accuracy: 0.9574 - val_loss: 0.4980 - val_accuracy: 0.8329\n",
      "Epoch 12/15\n",
      "7510/7510 [==============================] - 2s 243us/step - loss: 0.1333 - accuracy: 0.9583 - val_loss: 0.5091 - val_accuracy: 0.8472\n",
      "Epoch 13/15\n",
      "7510/7510 [==============================] - 2s 242us/step - loss: 0.1237 - accuracy: 0.9607 - val_loss: 0.5371 - val_accuracy: 0.8437\n",
      "Epoch 14/15\n",
      "7510/7510 [==============================] - 2s 244us/step - loss: 0.1119 - accuracy: 0.9643 - val_loss: 0.5845 - val_accuracy: 0.8329\n",
      "Epoch 15/15\n",
      "7510/7510 [==============================] - 2s 243us/step - loss: 0.1138 - accuracy: 0.9619 - val_loss: 0.5382 - val_accuracy: 0.8425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2118a1f05c8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the data into our model\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=15, batch_size=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b389e4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3219/3219 [==============================] - 0s 44us/step\n",
      "Accuracy: 84.25%\n",
      "loss: 0.5381908492411135\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Model Accuracy\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"loss: {}\".format((scores[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c0e186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle_file = open('sentiment_analysis_of_tweets.pkl', 'wb')     \n",
    "pickle.dump(model, pickle_file)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e2512",
   "metadata": {},
   "source": [
    "# PREDICTION FOR NEW TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f096e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  [0.83211845]\n",
      "Score:  [0.01057871]\n",
      "['Positive', 'Negative']\n"
     ]
    }
   ],
   "source": [
    "test_sample_1 = \"You are good\"\n",
    "test_sample_2 = \"You are bad\"\n",
    "test_samples = [test_sample_1, test_sample_2]\n",
    "\n",
    "test_sample_tokens = tokenizer.texts_to_sequences(d for d in test_samples)\n",
    "\n",
    "# Padding the testing sequences\n",
    "test_samples_tokens_pad = pad_sequences(test_sample_tokens, maxlen = 30, padding='post')\n",
    "\n",
    "scores = model.predict(x = test_samples_tokens_pad)\n",
    "\n",
    "def predict_tweet_sentiment(score):\n",
    "    print(\"Score: \", score)\n",
    "    return \"Positive\" if score > 0.5 else \"Negative\"\n",
    "\n",
    "model_predictions = [predict_tweet_sentiment(score) for score in scores]\n",
    "\n",
    "print(model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b17bc",
   "metadata": {},
   "source": [
    "# CONCLUSIONS, INSIGHTS AND RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4dba2d",
   "metadata": {},
   "source": [
    "1) Class balance is an important criterion when we are working on classification problems. It is essential to ensure that the classes are not very skewed, and the class imbalance will lead to biased results. \n",
    "Our dataset is quite imbalanced. The number of negative tweets are 4 times higher than the positive ones. Despite the fact that it's a biased model, our accuracy level seems high. But for various examples our model can fail predicting positive tweets.  \n",
    "\n",
    "For future improvements, sampling techniques can be applied to solve the imbalance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f500f91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0129921 ],\n",
       "       [0.01057871]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample_1 = \"You are wonderful\"\n",
    "test_sample_2 = \"You are bad\"\n",
    "test_samples = [test_sample_1, test_sample_2]\n",
    "\n",
    "test_sample_tokens = tokenizer.texts_to_sequences(d for d in test_samples)\n",
    "\n",
    "# Padding the testing sequences\n",
    "test_samples_tokens_pad = pad_sequences(test_sample_tokens, maxlen = 30, padding='post')\n",
    "\n",
    "model.predict(x = test_samples_tokens_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c74d3",
   "metadata": {},
   "source": [
    "2) Stemmization can be applied to improve predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b55ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
